{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from datasets import disable_caching, load_dataset\n",
    "import nltk\n",
    "from utils import get_user_system_assistant_format_messages, convert_qa_to_prompt_answer_format, convert_conversation_to_completion_format\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "nltk.download('punkt')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "disable_caching() # Saves a lot of harddisk memory (at the cost of no dataset caching) when dealing with the datasets library",
   "id": "21f37e3d68074850",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prompts to train on",
   "id": "10914cd9d2e88a97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "default_system_prompt = \"\"\"Du bist ein hilfsbereiter medizinischer KI-Assistent für die Notaufnahme.\"\"\"\n",
    "distillation_system_prompt = \"\"\"Du bist ein hilfsbereiter medizinischer KI-Assistent für die Notaufnahme. Im Folgenden findest du Leitlinienpassagen und mein Vorgehen bei der Behandlung.\n",
    "Deine Aufgabe ist es, zu prüfen, ob mein Vorgehen den Leitlinien gerecht wird.\n",
    "Beachte, dass die Behandlung in der Notaufnahme in zeitlich begrenztem Umfang stattfand. Vergleiche mein Vorgehen mit der Leitlinienpassage. Beurteile dabei ausschließlich den angegeben Behandlungsschritt.\n",
    "Falls mein Vorgehen im Kontext der Leitlinienpassage angemessen ist, antworte ausschließlich mit 'Ja'.\n",
    "Falls mein Vorgehen der Leitlinienpassage widerspricht, beginne deine Antwort mit 'Nein' und erkläre anschließend kurz und bündig, was ich hätte besser machen können.\n",
    "Falls zu der Leitlinie ein Rote-Hand-Brief vorliegt, antworte ausschließlich mit 'Ja'.\"\"\""
   ],
   "id": "eeade0aac717c33a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset Names",
   "id": "b438cbb8c63e71fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "english_dataset_names = ['FreedomIntelligence/Medical-R1-Distill-Data', 'FreedomIntelligence/medical-o1-reasoning-SFT', 'Laurent1/MedQuad-MedicalQnADataset_128tokens_max', 'lurosenb/medqa', 'qiaojin/PubMedQA', 'openlifescienceai/medmcqa', 'TIGER-Lab/MMLU-Pro'] # we don't use english datsets since we were not happy with the translation quality of free applications/models\n",
    "german_dataset_names = ['CausalLM/GPT-4-Self-Instruct-German', 'BioMistral/BioInstructQA', 'avemio/German-RAG-ORPO-Alpaca-HESSIAN-AI', 'amphora/Open-R1-Mulitlingual-SFT']"
   ],
   "id": "a8575eeeddd6ec98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## German Dataset Unification",
   "id": "8059ae58fba5781a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ger_data = load_dataset('CausalLM/GPT-4-Self-Instruct-German')",
   "id": "edf3ef78a5307dd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ger_data = ger_data['train']",
   "id": "4f4b6cc8e69f3cd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def bring_prompt_into_final_train_format(instruct:str,expected_response:str, system_prompt:str=default_system_prompt):\n",
    "    \"\"\"\n",
    "\n",
    "    :param instruct:\n",
    "    :param expected_response:\n",
    "    :param system_prompt:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    messages = get_user_system_assistant_format_messages(instruct, system_prompt, expected_response)\n",
    "    return messages"
   ],
   "id": "f243b48713e05c61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def apply_to_self_instruct_german(x):\n",
    "    \"\"\"\n",
    "\n",
    "    :param x:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x['conversation'] = bring_prompt_into_final_train_format(x['instruction'], x['output'])\n",
    "    return x"
   ],
   "id": "fc513d397b9f823e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ger_data = ger_data.map(apply_to_self_instruct_german)",
   "id": "a2aaf08bb9cba00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ger_data.to_pandas().to_feather(r'data\\distillation_training/auxiliary_datasets/gpt_4_self_instruct_german.feather')",
   "id": "99651ea09e81add5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load & Preprocess Dataset 2",
   "id": "ec21d7dc26e4837e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "# you have to download and extract this json file from https://huggingface.co/datasets/BioMistral/BioInstructQA/blob/main/data.zip\n",
    "with open(r'data\\distillation_training\\auxiliary_datasets\\German-full.json', encoding='utf-8') as f:\n",
    "    ger_data2 = json.loads(f.read())"
   ],
   "id": "4b440bf9d47c8b00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger2 = pd.DataFrame(ger_data2)",
   "id": "3621c1d09655eaaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_ger2['conversation'] = df_ger2.apply(\n",
    "    lambda row: (convert_qa_to_prompt_answer_format(\n",
    "        question=row['question_translated'],\n",
    "        answers=row['options_translated'],\n",
    "        correct_answer_letter=row['correct_answer_letter'])),\n",
    "    axis=1\n",
    ")\n"
   ],
   "id": "a5fc6ebba4fcc91b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger2 = df_ger2.explode('conversation')",
   "id": "a9a72b52f4b1a788",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger2[['conversation', 'question_translated', 'correct_answer_text_translated']].to_feather(r'data\\distillation_training/auxiliary_datasets/german_medqa.feather')",
   "id": "e2139e3776d21dc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load & Preprocess Dataset 3",
   "id": "f237a554a384a7b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "topics = ['hard-reasoning-de', 'SauerkrautLM-Fermented-GER-DPO', 'SauerkrautLM-Fermented-Irrelevance-GER-DPO', 'qa-meeting-attendee-topic', 'qa-meeting-topic', 'hard-qa-with-multiple-references']",
   "id": "ff46f275c2241d0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger_3 = pd.concat([load_dataset('avemio/German-RAG-ORPO-Alpaca-HESSIAN-AI', topic)['train'].to_pandas() for topic in topics]).reset_index(drop=True).dropna()",
   "id": "256ba7820541e191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger_3['conversation'] = df_ger_3.apply(lambda x: bring_prompt_into_final_train_format(x['Instruction'], x['Chosen'], x['System']), axis=1)",
   "id": "e908d1d4360f9ac2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger_3.to_feather(r'data\\distillation_training/auxiliary_datasets/avemio.feather')",
   "id": "8d910153e09f4780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load & Preprocess Dataset 4",
   "id": "f9a83e076a54a987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger_4 = pd.read_feather(r'data\\distillation_training/auxiliary_datasets/german_sft_r1.feather')",
   "id": "bddff9ec3c952259",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger_4['conversation'] = df_ger_4.apply(lambda x: bring_prompt_into_final_train_format(x['prompt'], x['response']), axis=1)",
   "id": "2e8d86136f6f56ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load & Preprocess Dataset 5",
   "id": "5b07aeaf0f1438e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger_5 = pd.concat([pd.read_parquet(path) for path in Path(r'data\\distillation_training\\auxiliary_datasets\\mmlu_de_medical').glob('*.parquet')])",
   "id": "cedd21af1c0213e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_ger_5['conversation'] = df_ger_5.apply(\n",
    "    lambda row: (convert_qa_to_prompt_answer_format(\n",
    "        question=row['question_de'],\n",
    "        answers={num: choice for num, choice in enumerate(row['choices_de'])},\n",
    "        correct_answer_letter=row['answer'])),\n",
    "    axis=1)\n"
   ],
   "id": "8f09345b9f84446c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger_5 = df_ger_5.explode('conversation')",
   "id": "3d1363f786aca366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load & Preprocess Dataset 6",
   "id": "8870735729f29d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger_6 = pd.concat([pd.read_parquet(path) for path in Path(r'data\\distillation_training\\auxiliary_datasets\\mmlu_de_other').glob('*.parquet')])",
   "id": "ab98939a10f5444a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_ger_6['conversation'] = df_ger_6.apply(\n",
    "    lambda row: (convert_qa_to_prompt_answer_format(\n",
    "        question=row['question_de'],\n",
    "        answers={num: choice for num, choice in enumerate(row['choices_de'])},\n",
    "        correct_answer_letter=row['answer'])),\n",
    "    axis=1)\n",
    "\n",
    "df_ger_6 = df_ger_6.explode('conversation')"
   ],
   "id": "6bcbf20331fc54b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_ger = ger_data.to_pandas()",
   "id": "5d42e7290043f9b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Add metadata & Combine Datasets",
   "id": "b95ca48b5286753e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_ger['source'] = german_dataset_names[0]\n",
    "df_ger2['source'] = german_dataset_names[1]\n",
    "df_ger_3['source'] = german_dataset_names[2]\n",
    "df_ger_4['source'] = german_dataset_names[3]\n",
    "df_ger_5['source'] = 'MMLU_de_Medical'\n",
    "df_ger_6['source'] = 'MMLU_de_Other'"
   ],
   "id": "bbb9bc2baebf9560",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final = pd.concat([df_ger[['source', 'conversation']], df_ger2[['source', 'conversation']], df_ger_3[['source', 'conversation']],df_ger_4[['source', 'conversation']], df_ger_5[['source', 'conversation']], df_ger_6[['source', 'conversation']]])",
   "id": "338e8f5237c7d61b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analysis of final dataset",
   "id": "5aa6c9394adf7198"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final.shape",
   "id": "35252446292cdd1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final['source'].value_counts()",
   "id": "c4c96d7c5bb24d4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final['training_data_type'] = df_final['source'].apply(lambda x: 'medical' if x in ['BioMistral/BioInstructQA', 'MMLU_de_Medical'] else 'other')",
   "id": "8f4b4253e803b350",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final['training_data_type'].value_counts()",
   "id": "e49553454e091f37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert Dataset to Completion format",
   "id": "fa366b5fb2a6a2dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final = convert_conversation_to_completion_format(df_final)",
   "id": "f588401a7c2df572",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final.to_feather(r'data\\distillation_training\\auxiliary_datasets\\all_german_data_combined_completion_format.feather')",
   "id": "1afdc16b3df13156",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
