{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:20.571814Z",
     "start_time": "2025-07-20T08:22:16.317806Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from scipy.special import kl_div\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Any\n",
    "from utils import clear_cuda, load_benchmark_prompts_and_answers\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, balanced_accuracy_score\n",
    "from data.Benchmark.benchmark import benchmark_and_evaluate_models"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:20.775545Z",
     "start_time": "2025-07-20T08:22:20.772072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llama_models = ['meta-llama/Llama-3.1-8B-Instruct',\n",
    "                'unsloth/Llama-3.1-8B-Instruct-bnb-4bit',\n",
    "                'hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4']\n",
    "unquantized_model = llama_models[0]\n",
    "precisions = ['bfloat16', 'int4', 'int4']\n",
    "precision_map = dict(zip(llama_models, precisions))"
   ],
   "id": "7fe5f9f983b57663",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:20.856064Z",
     "start_time": "2025-07-20T08:22:20.852511Z"
    }
   },
   "cell_type": "code",
   "source": "prompts, answers = load_benchmark_prompts_and_answers()",
   "id": "e928b58a3b6df47",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:27:08.310003Z",
     "start_time": "2025-07-20T08:22:20.878323Z"
    }
   },
   "cell_type": "code",
   "source": "results = benchmark_and_evaluate_models(llama_models,prompts,answers, precision_map, unquantized_model)",
   "id": "35545b3094fcaa37",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c88001fccac74bebb4b5b566ea23700f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\Thesis\\.venv\\Lib\\site-packages\\torch\\__init__.py:1117: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n",
      "D:\\PycharmProjects\\Thesis\\.venv\\Lib\\site-packages\\auto_gptq\\nn_modules\\triton_utils\\kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "D:\\PycharmProjects\\Thesis\\.venv\\Lib\\site-packages\\auto_gptq\\nn_modules\\triton_utils\\kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "D:\\PycharmProjects\\Thesis\\.venv\\Lib\\site-packages\\auto_gptq\\nn_modules\\triton_utils\\kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c69eceef38d4a7c93268749e76b7805"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4 were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:27:16.626473Z",
     "start_time": "2025-07-20T08:27:10.179418Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "f8cf042055878895",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'benchmark_df':                                                   model  \\\n",
       " 0                      meta-llama/Llama-3.1-8B-Instruct   \n",
       " 1                      meta-llama/Llama-3.1-8B-Instruct   \n",
       " 2                      meta-llama/Llama-3.1-8B-Instruct   \n",
       " 3                      meta-llama/Llama-3.1-8B-Instruct   \n",
       " 4                      meta-llama/Llama-3.1-8B-Instruct   \n",
       " ...                                                 ...   \n",
       " 2035  hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ...   \n",
       " 2036  hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ...   \n",
       " 2037  hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ...   \n",
       " 2038  hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ...   \n",
       " 2039  hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ...   \n",
       " \n",
       "                                           probabilities answer  \\\n",
       " 0     [[tensor(6.1118e-10, device='cuda:0', dtype=to...     Ja   \n",
       " 1     [[tensor(1.0277e-10, device='cuda:0', dtype=to...     Ne   \n",
       " 2     [[tensor(9.8225e-10, device='cuda:0', dtype=to...     Ja   \n",
       " 3     [[tensor(5.8571e-10, device='cuda:0', dtype=to...     Ne   \n",
       " 4     [[tensor(5.9299e-10, device='cuda:0', dtype=to...     Ne   \n",
       " ...                                                 ...    ...   \n",
       " 2035  [[tensor(7.3487e-10, device='cuda:0', dtype=to...     Ne   \n",
       " 2036  [[tensor(5.5297e-10, device='cuda:0', dtype=to...     Ne   \n",
       " 2037  [[tensor(4.3110e-10, device='cuda:0', dtype=to...     Ne   \n",
       " 2038  [[tensor(4.1655e-10, device='cuda:0', dtype=to...     Ne   \n",
       " 2039  [[tensor(3.3469e-09, device='cuda:0', dtype=to...     Ja   \n",
       " \n",
       "       answer_confidence                                top_5_probabilities  \\\n",
       " 0              0.972656  {'Ja': 0.97265625, 'Ne': 0.02587890625, 'Yes':...   \n",
       " 1              1.000000  {'Ne': 1.0, 'Ja': 0.00048828125, 'Ke': 3.52859...   \n",
       " 2              0.589844  {'Ja': 0.58984375, 'Ne': 0.40625, 'Yes': 0.000...   \n",
       " 3              0.703125  {'Ne': 0.703125, 'Ja': 0.29296875, 'Yes': 0.00...   \n",
       " 4              0.648438  {'Ne': 0.6484375, 'Ja': 0.34765625, 'Yes': 0.0...   \n",
       " ...                 ...                                                ...   \n",
       " 2035           1.000000  {'Ne': 1.0, 'Ja': 0.0002307891845703125, 'N': ...   \n",
       " 2036           1.000000  {'Ne': 1.0, 'Ja': 5.14984130859375e-05, 'N': 1...   \n",
       " 2037           1.000000  {'Ne': 1.0, 'Ja': 0.000431060791015625, 'N': 1...   \n",
       " 2038           0.996094  {'Ne': 0.99609375, 'Ja': 0.005218505859375, 'N...   \n",
       " 2039           0.816406  {'Ja': 0.81640625, 'Ne': 0.1826171875, 'Ein': ...   \n",
       " \n",
       "                                                question correct_answer  \\\n",
       " 0     [{'content': 'Du bist ein medizinischer Expert...             Ja   \n",
       " 1     [{'content': 'Du bist ein medizinischer Expert...           Nein   \n",
       " 2     [{'content': 'Du bist ein medizinischer Expert...           Nein   \n",
       " 3     [{'content': 'Du bist ein medizinischer Expert...           Nein   \n",
       " 4     [{'content': 'Du bist ein medizinischer Expert...           Nein   \n",
       " ...                                                 ...            ...   \n",
       " 2035  [{'content': 'Du bist ein medizinischer Expert...           Nein   \n",
       " 2036  [{'content': 'Du bist ein medizinischer Expert...           Nein   \n",
       " 2037  [{'content': 'Du bist ein medizinischer Expert...             Ja   \n",
       " 2038  [{'content': 'Du bist ein medizinischer Expert...           Nein   \n",
       " 2039  [{'content': 'Du bist ein medizinischer Expert...           Nein   \n",
       " \n",
       "      model_precision  bool_answer  correct_bool_answer  \n",
       " 0           bfloat16         True                 True  \n",
       " 1           bfloat16        False                False  \n",
       " 2           bfloat16         True                False  \n",
       " 3           bfloat16        False                False  \n",
       " 4           bfloat16        False                False  \n",
       " ...              ...          ...                  ...  \n",
       " 2035            int4        False                False  \n",
       " 2036            int4        False                False  \n",
       " 2037            int4        False                 True  \n",
       " 2038            int4        False                False  \n",
       " 2039            int4         True                False  \n",
       " \n",
       " [2040 rows x 10 columns],\n",
       " 'classification_metrics': {'meta-llama/Llama-3.1-8B-Instruct': {'accuracy': 0.6529411764705882,\n",
       "   'balanced_accuracy': 0.6397058823529411,\n",
       "   'cohen_kappa': 0.20485175202156336},\n",
       "  'unsloth/Llama-3.1-8B-Instruct-bnb-4bit': {'accuracy': 0.6764705882352942,\n",
       "   'balanced_accuracy': 0.6075367647058824,\n",
       "   'cohen_kappa': 0.17541229385307355},\n",
       "  'hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4': {'accuracy': 0.7808823529411765,\n",
       "   'balanced_accuracy': 0.5762867647058824,\n",
       "   'cohen_kappa': 0.18221734357848518},\n",
       "  'always_false_baseline': {'accuracy': 0.8,\n",
       "   'balanced_accuracy': 0.5,\n",
       "   'cohen_kappa': 0.0}},\n",
       " 'kl_divergence': {'unsloth/Llama-3.1-8B-Instruct-bnb-4bit': np.float32(0.07237416),\n",
       "  'hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4': np.float32(0.3089346)},\n",
       " 'hellinger_distance': {'unsloth/Llama-3.1-8B-Instruct-bnb-4bit': np.float64(0.0995063487363991),\n",
       "  'hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4': np.float64(0.2423799285647464)}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:18:45.490439Z",
     "start_time": "2025-07-20T08:18:45.484728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def benchmark_model(data_dict:dict[str,list[Any]],used_model:Any, used_tokenizer:Any, benchmark_prompts:list[str], benchmark_answers:list[str], model_precision:str, model_name:str) -> dict[str,Any]:\n",
    "    \"\"\"\n",
    "    Benchmarks a language model's top-1 and top-5 predictions across a set of prompt-answer pairs.\n",
    "\n",
    "    For each prompt in `benchmark_prompts`, the model is used to generate a single-token response.\n",
    "    The top 5 predicted tokens and their probabilities are collected and top-1 accuracy is estimated\n",
    "    by comparing the highest-probability token to the expected answer.\n",
    "\n",
    "    The results are appended to the `data_dict`, which should already be initialized with keys:\n",
    "        - 'model', 'probabilities', 'answer', 'answer_confidence', 'top_5_probabilities',\n",
    "          'question', 'correct_answer', 'model_precision'\n",
    "\n",
    "    :param data_dict: Dictionary used to collect benchmark results.\n",
    "    :param used_model: The preloaded language model (must support `.eval()` and `.logits` output).\n",
    "    :param used_tokenizer: The tokenizer corresponding to the model (must support chat template formatting).\n",
    "    :param benchmark_prompts: A list of prompts/questions to pass to the model.\n",
    "    :param benchmark_answers: A list of correct answers, matched to `benchmark_prompts`.\n",
    "    :param model_precision: A string identifier for the model's precision type (e.g., \"fp16\", \"int8\").\n",
    "    :param model_name: The name of the benchmarked language model.\n",
    "    :return: Updated `data_dict` containing predictions and metadata for all evaluated prompts.\n",
    "    \"\"\"\n",
    "    used_model.eval()\n",
    "    with torch.inference_mode(): # disables gradient calculations, dropout and other training only calculations/settings\n",
    "        for prompt, expected_answer in zip(benchmark_prompts, benchmark_answers):\n",
    "            inputs = used_tokenizer.apply_chat_template(\n",
    "                prompt,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "                return_dict=True,\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            next_token_logits = used_model(**inputs).logits[:, -1, :]\n",
    "            probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "            topk_probs, topk_indices = torch.topk(probabilities, k=5)\n",
    "            tokens = used_tokenizer.convert_ids_to_tokens(topk_indices.tolist()[0])\n",
    "            top_5_token_probabilities = dict(zip(tokens, topk_probs.squeeze().tolist()))\n",
    "            data_dict['model'].append(model_name)\n",
    "            data_dict['probabilities'].append(probabilities)\n",
    "            data_dict['answer'].append(tokens[0])\n",
    "            data_dict['answer_confidence'].append(top_5_token_probabilities[tokens[0]])\n",
    "            data_dict['top_5_probabilities'].append(top_5_token_probabilities)\n",
    "            data_dict['question'].append(prompt)\n",
    "            data_dict['correct_answer'].append(expected_answer)\n",
    "            data_dict['model_precision'].append(model_precision)\n",
    "    return data_dict"
   ],
   "id": "3f2b7491bdb03fe3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-07-20T08:18:47.441316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {'model': [], 'probabilities': [], 'answer': [], 'top_5_probabilities': [], 'question': [], 'correct_answer': [], 'answer_confidence': [], 'model_precision': []}\n",
    "for model_name, precision in zip(llama_models, precisions):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map='cuda')\n",
    "    data = benchmark_model(data,model, tokenizer, prompts, answers, precision, model_name)\n",
    "    del model, tokenizer\n",
    "    clear_cuda()"
   ],
   "id": "51ac7ec25f13d1a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79719fae04c64d4f9466e738f65af238"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:10.776580600Z",
     "start_time": "2025-07-19T07:14:17.912841Z"
    }
   },
   "cell_type": "code",
   "source": "df_quantization_benchmark = pd.DataFrame.from_dict(data)",
   "id": "8fe676ccc80c7934",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#df_quantization_benchmark.to_feather('quantization_benchmark_raw_data.feather')",
   "id": "1d0d6351ad7a447e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:12.352304300Z",
     "start_time": "2025-07-19T07:33:34.578317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_answer_to_bool(answer:str) -> bool|float:\n",
    "    \"\"\"\n",
    "    Converts the answer to a boolean value or NaN.\n",
    "\n",
    "    :param answer: String of model answer to a yes/no question.\n",
    "    :return: True if answer starts with 'j' or 'J',\n",
    "             False if it starts with 'n' or 'N',\n",
    "             np.nan otherwise.\n",
    "    \"\"\"\n",
    "    if answer.lower().startswith('j'):\n",
    "        return True\n",
    "    elif answer.lower().startswith('n'):\n",
    "        return False\n",
    "    else:\n",
    "        return np.nan\n"
   ],
   "id": "1081042c409ec05",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:12.357304600Z",
     "start_time": "2025-07-19T07:39:32.407772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_quantization_benchmark['bool_answer'] = df_quantization_benchmark['answer'].apply(convert_answer_to_bool)\n",
    "df_quantization_benchmark['correct_bool_answer'] = df_quantization_benchmark['correct_answer'].apply(convert_answer_to_bool)"
   ],
   "id": "72d5cbb4f75fbc97",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:12.478036Z",
     "start_time": "2025-07-19T07:34:34.776218Z"
    }
   },
   "cell_type": "code",
   "source": "df_quantization_benchmark.columns",
   "id": "4e7948cdc03d1f89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model', 'probabilities', 'answer', 'top_5_probabilities', 'question',\n",
       "       'correct_answer', 'answer_confidence', 'model_precision', 'bool_answer',\n",
       "       'expected_bool_answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:12.480038200Z",
     "start_time": "2025-07-19T08:55:58.878807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_models(df:pd.DataFrame, model_list:list[str], include_baseline:bool=True):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance metrics for a list of models using a DataFrame of predictions.\n",
    "\n",
    "    For each model in the provided list, this function computes:\n",
    "      - Accuracy\n",
    "      - Balanced accuracy\n",
    "      - Cohen's kappa score\n",
    "\n",
    "    Optionally, it also evaluates a baseline model that always predicts `False`, using the same\n",
    "    subset of data as the first model in `model_list`.\n",
    "\n",
    "    :param df: A pandas DataFrame containing model predictions. Must include the columns:\n",
    "               - 'model': model identifier\n",
    "               - 'correct_bool_answer': ground truth boolean labels\n",
    "               - 'bool_answer': model's predicted boolean labels\n",
    "    :param model_list: A list of model names (as strings) to evaluate.\n",
    "    :param include_baseline: If True, evaluates a baseline model that always predicts False.\n",
    "    :return: A dictionary where keys are model names and values are dictionaries containing:\n",
    "             - 'accuracy': standard accuracy score\n",
    "             - 'balanced_accuracy': balanced accuracy score\n",
    "             - 'cohen_kappa': Cohen's kappa score\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for model_name in model_list:\n",
    "        model_df = df[df['model'] == model_name]\n",
    "        y_true = model_df['correct_bool_answer']\n",
    "        y_pred = model_df['bool_answer']\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "        results[model_name] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"balanced_accuracy\": balanced_acc,\n",
    "            \"cohen_kappa\": kappa\n",
    "        }\n",
    "\n",
    "    if include_baseline:\n",
    "        baseline_df = df[df['model'] == llama_models[0]].copy()\n",
    "        baseline_df['bool_answer'] = False\n",
    "        y_true = baseline_df['correct_bool_answer']\n",
    "        y_pred = baseline_df['bool_answer']\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "        results[\"always_false_baseline\"] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"balanced_accuracy\": balanced_acc,\n",
    "            \"cohen_kappa\": kappa\n",
    "        }\n",
    "\n",
    "    return results"
   ],
   "id": "c908320f9080caa8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for meta-llama/Llama-3.1-8B-Instruct:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.66      0.75       544\n",
      "        True       0.31      0.62      0.42       136\n",
      "\n",
      "    accuracy                           0.65       680\n",
      "   macro avg       0.59      0.64      0.58       680\n",
      "weighted avg       0.76      0.65      0.69       680\n",
      "\n",
      "Balanced Accuracy: 0.6397\n",
      "Cohen's Kappa: 0.2049\n",
      "------------------------------------------------------------\n",
      "Classification report for unsloth/Llama-3.1-8B-Instruct-bnb-4bit:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.85      0.72      0.78       544\n",
      "        True       0.31      0.49      0.38       136\n",
      "\n",
      "    accuracy                           0.68       680\n",
      "   macro avg       0.58      0.61      0.58       680\n",
      "weighted avg       0.74      0.68      0.70       680\n",
      "\n",
      "Balanced Accuracy: 0.6075\n",
      "Cohen's Kappa: 0.1754\n",
      "------------------------------------------------------------\n",
      "Classification report for hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.83      0.92      0.87       544\n",
      "        True       0.42      0.24      0.30       136\n",
      "\n",
      "    accuracy                           0.78       680\n",
      "   macro avg       0.62      0.58      0.59       680\n",
      "weighted avg       0.75      0.78      0.76       680\n",
      "\n",
      "Balanced Accuracy: 0.5763\n",
      "Cohen's Kappa: 0.1822\n",
      "------------------------------------------------------------\n",
      "Classification report for always guessing False:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      1.00      0.89       544\n",
      "        True       0.00      0.00      0.00       136\n",
      "\n",
      "    accuracy                           0.80       680\n",
      "   macro avg       0.40      0.50      0.44       680\n",
      "weighted avg       0.64      0.80      0.71       680\n",
      "\n",
      "Balanced Accuracy: 0.5000\n",
      "Cohen's Kappa: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PycharmProjects\\Thesis\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\PycharmProjects\\Thesis\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "D:\\PycharmProjects\\Thesis\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "eval_results = evaluate_models(df_quantization_benchmark, llama_models)",
   "id": "6a91a62ca007d512"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_distribution_divergences(\n",
    "    df: pd.DataFrame,\n",
    "    llama_models: list[str],\n",
    "    unquantized_model: str\n",
    ") -> tuple[dict[str, float], dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Computes the KL divergence and Hellinger distance between the probability distributions\n",
    "    of quantized models and a reference unquantized model.\n",
    "\n",
    "    :param df: DataFrame containing a 'model' column and a 'probabilities' column (each entry is a tensor).\n",
    "    :param llama_models: List of model names to compare.\n",
    "    :param unquantized_model: Name of the reference (unquantized) model.\n",
    "    :return: Tuple of two dictionaries:\n",
    "             - KL divergence results: {model_name: mean_kl_divergence}\n",
    "             - Hellinger distance results: {model_name: mean_hellinger_distance}\n",
    "    \"\"\"\n",
    "    ref_probs = df[df['model'] == unquantized_model]['probabilities'].to_list()\n",
    "    ref_probs = torch.stack(ref_probs, dim=-1).detach().cpu().float().squeeze().numpy()\n",
    "\n",
    "    kl_results = {}\n",
    "    hellinger_results = {}\n",
    "\n",
    "    for model_name in llama_models:\n",
    "        if model_name == unquantized_model:\n",
    "            continue\n",
    "\n",
    "        data_subset = df[df['model'] == model_name]\n",
    "        comp_probs = data_subset['probabilities'].to_list()\n",
    "        comp_probs = torch.stack(comp_probs, dim=-1).detach().cpu().float().squeeze().numpy()\n",
    "\n",
    "        if ref_probs.shape != comp_probs.shape:\n",
    "            print(f\"Shape mismatch between {unquantized_model} and {model_name}\")\n",
    "            continue\n",
    "\n",
    "        model_kl_divs = np.sum(kl_div(comp_probs, ref_probs), axis=0)\n",
    "        sqrt_diff = np.sqrt(comp_probs) - np.sqrt(ref_probs)\n",
    "        hellinger_distance = np.sqrt(np.sum(sqrt_diff ** 2, axis=0)) / np.sqrt(2)\n",
    "\n",
    "        kl_results[model_name] = np.mean(model_kl_divs)\n",
    "        hellinger_results[model_name] = np.mean(hellinger_distance)\n",
    "\n",
    "    return kl_results, hellinger_results"
   ],
   "id": "be3bfe2f33eecf8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:12.560294800Z",
     "start_time": "2025-07-19T08:54:05.866941Z"
    }
   },
   "cell_type": "code",
   "source": "kl_results, hellinger_results = compute_distribution_divergences(df_quantization_benchmark, llama_models, unquantized_model)\n",
   "id": "1ba0a81b400606fa",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:12.561293500Z",
     "start_time": "2025-07-19T08:09:50.169176Z"
    }
   },
   "cell_type": "code",
   "source": "kl_results",
   "id": "45938b3c6a132de5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unsloth/Llama-3.1-8B-Instruct-bnb-4bit': np.float32(0.07237416),\n",
       " 'hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4': np.float32(0.3089346)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:22:12.601323800Z",
     "start_time": "2025-07-19T08:18:57.524171Z"
    }
   },
   "cell_type": "code",
   "source": "hellinger_results",
   "id": "bec39ba597b44fd6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unsloth/Llama-3.1-8B-Instruct-bnb-4bit': np.float64(0.0995063487363991),\n",
       " 'hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4': np.float64(0.2423799285647464)}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
