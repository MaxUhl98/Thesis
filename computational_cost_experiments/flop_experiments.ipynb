{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoConfig\n",
    "import torch\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "from utils import ComputeClient\n",
    "import time\n",
    "from utils import get_example_prompt\n",
    "import pandas as pd\n",
    "import random"
   ],
   "id": "36df9a8420c34147"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "device = 'cuda' if torch.cuda.is_available() else 'cpu'",
   "id": "860f64d108dea9a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T07:51:52.216424Z",
     "start_time": "2025-07-21T07:51:52.213344Z"
    }
   },
   "cell_type": "code",
   "source": "torch.set_default_device(device)",
   "id": "184105541d7a20ac",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T07:51:52.234877Z",
     "start_time": "2025-07-21T07:51:52.232573Z"
    }
   },
   "cell_type": "code",
   "source": "model_name = \"D:\\models\\Huggingface\\hub\\models--meta-llama--Llama-3.1-8B-Instruct\"  #'meta-llama/Llama-3.1-8B-Instruct'",
   "id": "7a9f52b917490b68",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T07:51:52.396363Z",
     "start_time": "2025-07-21T07:51:52.311674Z"
    }
   },
   "cell_type": "code",
   "source": "model_config = AutoConfig.from_pretrained(model_name)",
   "id": "9401e74e04fb69cb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T07:53:19.149894Z",
     "start_time": "2025-07-21T07:53:19.145700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenization cost is assumed to scale linearly with sequence length.\n",
    "# Since the exact multiplicative constant and additive bias are unknown,\n",
    "# we assume a zero bias and adopt a conservatively large constant (200).\n",
    "# This overestimation is acceptable, as tokenization FLOPs are expected\n",
    "# to be negligible compared to the quadratic and higher-order operations\n",
    "# in transformer layers.\n",
    "guessed_tokenization_constant = 1\n",
    "flash_attention_version = 3  # 3 is the only Flash Attention Version implemented on SGLang\n",
    "flash_attn_efficiencies = {1: .325, 2: .615, 3: .75}\n",
    "GPU_FLOPS = 165.2 * 10 ** 12  # FLOPS of Nvidia RTX 4090 in float16\n",
    "GPU_BANDWIDTH = 1008 * 1024 ** 3"
   ],
   "id": "91e8b55c65f18453",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "client = ComputeClient()",
   "id": "8d8a5d4c3b5b29f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_ffn_flops(batch_size: int, sequence_length: int, d_i: int, d_m: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the FLOPs required for a Feed-Forward Network (FFN) layer using a SwiGLU activation.\n",
    "\n",
    "    The formula used is:\n",
    "        FLOP_{FFN} = batch_size * sequence_length * d_inner * (10 + 3 * d_model)\n",
    "\n",
    "    :param batch_size: Number of input sequences processed in parallel.\n",
    "    :param sequence_length: Number of tokens per sequence.\n",
    "    :param d_i: Inner dimension of the FFN (also referred to as d_inner).\n",
    "    :param d_m: Hidden dimension of the model (also referred to as d_model).\n",
    "    :return: Total number of FLOPs for the FFN layer.\n",
    "    \"\"\"\n",
    "    return batch_size * sequence_length * d_i * (10 + 5 * d_m)\n",
    "\n",
    "\n",
    "def calculate_rms_flops(batch_size: int, sequence_length: int, d_m: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the FLOPs required for an RMSNorm operation.\n",
    "\n",
    "    The formula used is:\n",
    "        FLOP_{RMS}^{Batch} = batch_size * sequence_length * (4 * d_model + 3)\n",
    "\n",
    "    :param batch_size: Number of input sequences processed in parallel.\n",
    "    :param sequence_length: Number of tokens per sequence.\n",
    "    :param d_m: Model's hidden dimension (d_model).\n",
    "    :return: Total FLOPs for one RMSNorm operation over the batch.\n",
    "    \"\"\"\n",
    "    return batch_size * sequence_length * (4 * d_m + 3)\n",
    "\n",
    "\n",
    "def calculate_transformer_block_flops(batch_size: int, sequence_length: int, d_i: int, d_m: int, n_g: int,\n",
    "                                      d_h: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the total FLOPs for a single transformer block as used in LLaMA 3.1 8B.\n",
    "\n",
    "    Formula derived from the following components:\n",
    "        F_TB = 2 * F_skip + 2 * F_RMS + F_RoPE + F_Projection + F_Attention + F_FFN\n",
    "\n",
    "        Which simplifies to:\n",
    "        F_TB = batch_size * sequence_length * (\n",
    "            6 + 10 * d_i + d_m * (15 + 2 * sequence_length + d_m + 6 * d_h * n_g + 3 * d_i)\n",
    "        )\n",
    "\n",
    "    :param batch_size: Number of sequences in the batch.\n",
    "    :param sequence_length: Number of tokens per sequence.\n",
    "    :param d_i: Inner FFN dimension.\n",
    "    :param d_m: Model (hidden) dimension.\n",
    "    :param n_g: Number of attention groups (GQA setup).\n",
    "    :param d_h: Dimension of each attention head.\n",
    "    :return: Total number of FLOPs for a single transformer block.\n",
    "    \"\"\"\n",
    "    return batch_size * sequence_length * (\n",
    "            d_m * (2 * d_m * n_g + 6 * d_h * n_g + 5 * d_i + 9) + n_g * d_h * (\n",
    "                4 * sequence_length - 1) + 10 * d_i + 5 * n_g * sequence_length + 3\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_final_linear_flops(batch_size: int, sequence_length: int, d_m: int, vocab_size: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the FLOPs required for the final linear projection to vocabulary logits.\n",
    "\n",
    "    The formula used is:\n",
    "        FLOP = batch_size * (2 * sequence_length * d_model * vocab_size)\n",
    "\n",
    "    :param batch_size: Number of sequences processed in parallel.\n",
    "    :param sequence_length: Number of tokens per sequence.\n",
    "    :param d_m: Model's hidden dimension (d_model).\n",
    "    :param vocab_size: Size of the vocabulary used for prediction.\n",
    "    :return: Total FLOPs for the final linear transformation.\n",
    "    \"\"\"\n",
    "    return batch_size * 2 * sequence_length * d_m * vocab_size\n",
    "\n",
    "\n",
    "def calculate_final_softmax_flops(batch_size: int, vocab_size: int) -> int:\n",
    "    \"\"\"\n",
    "    Estimate the FLOPs for the final softmax layer applied only to the last token of each sequence.\n",
    "\n",
    "    According to TensorFlow Profiler, softmax costs approximately 5 * vocab_size FLOPs per call.\n",
    "\n",
    "    :param batch_size: Number of sequences processed in parallel.\n",
    "    :param vocab_size: Number of logits per token (typically equals vocabulary size).\n",
    "    :return: Total FLOPs for softmax applied to the final token in each sequence.\n",
    "    \"\"\"\n",
    "    return batch_size * 5 * vocab_size\n",
    "\n",
    "\n",
    "def calculate_theoretical_flop_count(input_shape: tuple[int, int], model_config: LlamaConfig,\n",
    "                                     tokenization_constant: int) -> int:\n",
    "    \"\"\"\n",
    "    Estimate the total theoretical number of floating-point operations (FLOPs) required to\n",
    "    generate a sequence using a LLaMA-style transformer architecture.\n",
    "\n",
    "    This includes the FLOPs for:\n",
    "        - Tokenization (approximate cost per token)\n",
    "        - All transformer blocks (multi-head attention, FFN, normalization, etc.)\n",
    "        - Final linear projection to logits\n",
    "        - Final softmax operation (applied to the last token of each sequence)\n",
    "\n",
    "    :param input_shape: Tuple of shape (batch_size, sequence_length), representing the\n",
    "                        number of sequences processed in parallel and the number of\n",
    "                        tokens generated per sequence.\n",
    "    :param model_config: Configuration object containing model hyperparameters. Expected to have:\n",
    "        - vocab_size (int): Size of the vocabulary used for output predictions.\n",
    "        - n_layers (int): Number of transformer blocks in the model.\n",
    "        - hidden_size (int): Dimensionality of the model's hidden states (d_model).\n",
    "        - intermediate_size (int): Dimensionality of the FFN's inner layer (d_inner).\n",
    "        - num_attention_heads (int): Total number of attention heads (n_h).\n",
    "        - num_key_value_heads (int): Number of key/value attention groups (n_g, for GQA).\n",
    "    :param tokenization_constant: Linear multiplier for tokenization cost (we only know that it is linear, but constant and bias are unknown)\n",
    "    :return: Total estimated FLOP count as an integer for generating one batch of sequences.\n",
    "    \"\"\"\n",
    "    b = input_shape[0]\n",
    "    s_L = input_shape[1]\n",
    "    d_m = model_config.hidden_size\n",
    "    d_i = model_config.intermediate_size\n",
    "    n_g = model_config.num_key_value_heads\n",
    "    d_h = model_config.head_dim\n",
    "    L_v = model_config.vocab_size\n",
    "    n_blocks = model_config.num_hidden_layers\n",
    "\n",
    "    f_tokenization = tokenization_constant * s_L\n",
    "    f_transformer_total = n_blocks * calculate_transformer_block_flops(b, s_L, d_i, d_m, n_g, d_h)\n",
    "    f_final_linear = calculate_final_linear_flops(b, s_L, d_m, L_v)\n",
    "    f_final_softmax = calculate_final_softmax_flops(b, L_v)\n",
    "\n",
    "    return int(f_tokenization + f_transformer_total + f_final_linear + f_final_softmax)\n"
   ],
   "id": "8ec7751d2993d017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def permute_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Randomly permutes the input string s.\n",
    "    :param s: String to be permuted\n",
    "    :return: Permuted String\n",
    "    \"\"\"\n",
    "    chars = list(s)\n",
    "    random.shuffle(chars)\n",
    "    return ''.join(chars)"
   ],
   "id": "981ae71bb93fb661"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T07:51:52.535594Z",
     "start_time": "2025-07-21T07:51:52.533097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_sequence(length: int, pattern='$§') -> str:\n",
    "    repeated = (pattern * ((length // len(pattern)) + 1))[:length]\n",
    "    return repeated"
   ],
   "id": "fae0142e9c2f8454",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:00:21.911269Z",
     "start_time": "2025-07-18T10:00:21.908269Z"
    }
   },
   "cell_type": "code",
   "source": "sequence_lengths = [1, 128, 256, 512, 1024, 2048, 4096] + [1366, 5264, 4677, 7151, 5875]\n",
   "id": "24dceb57f81a7f9a",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:14:52.495609Z",
     "start_time": "2025-07-21T08:14:52.181997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "effective_flops = GPU_FLOPS * flash_attn_efficiencies[flash_attention_version]\n",
    "data = {'sequence_length': [], 'calculated_flops': [], 'estimated_inference_time': [], 'inference_time': [],\n",
    "        'time_delta': [], 'delta_percentage': []}\n",
    "for seq_len in sequence_lengths:\n",
    "    input_sequence = create_sequence(seq_len)\n",
    "    data['sequence_length'].append(seq_len)\n",
    "    estimated_flops = calculate_theoretical_flop_count((1, seq_len), model_config, guessed_tokenization_constant)\n",
    "    data['calculated_flops'].append(estimated_flops)\n",
    "    estimated_time = estimated_flops / effective_flops\n",
    "    estimated_memory_time = estimated_flops / GPU_BANDWIDTH\n",
    "    data['estimated_inference_time'].append(estimated_time)\n",
    "    t_0 = time.time()\n",
    "    client.call_llm('', input_sequence, {'max_new_tokens': 1, 'temperature': 0})\n",
    "    t_final = time.time()\n",
    "    time_taken = t_final - t_0\n",
    "    delta = time_taken - estimated_time\n",
    "    data['time_delta'].append(delta)\n",
    "    data['inference_time'].append(time_taken)\n",
    "    data['delta_percentage'].append(abs(delta / time_taken))\n",
    "    client.call_llm('This is sent to overwrite your cache', 'This should make the cache useless.',\n",
    "                    {'max_new_tokens': 1, 'temperature': 0})\n",
    "\n"
   ],
   "id": "d19ca31b745e5295",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequence_lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[89]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m effective_flops = GPU_FLOPS * flash_attn_efficiencies[flash_attention_version]\n\u001B[32m      2\u001B[39m data = {\u001B[33m'\u001B[39m\u001B[33msequence_length\u001B[39m\u001B[33m'\u001B[39m: [], \u001B[33m'\u001B[39m\u001B[33mcalculated_flops\u001B[39m\u001B[33m'\u001B[39m: [], \u001B[33m'\u001B[39m\u001B[33mestimated_inference_time\u001B[39m\u001B[33m'\u001B[39m: [], \u001B[33m'\u001B[39m\u001B[33minference_time\u001B[39m\u001B[33m'\u001B[39m: [],\n\u001B[32m      3\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mtime_delta\u001B[39m\u001B[33m'\u001B[39m: [], \u001B[33m'\u001B[39m\u001B[33mdelta_percentage\u001B[39m\u001B[33m'\u001B[39m: []}\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m seq_len \u001B[38;5;129;01min\u001B[39;00m \u001B[43msequence_lengths\u001B[49m:\n\u001B[32m      5\u001B[39m     input_sequence = create_sequence(seq_len)\n\u001B[32m      6\u001B[39m     data[\u001B[33m'\u001B[39m\u001B[33msequence_length\u001B[39m\u001B[33m'\u001B[39m].append(seq_len)\n",
      "\u001B[31mNameError\u001B[39m: name 'sequence_lengths' is not defined"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:04:32.735672Z",
     "start_time": "2025-07-18T10:04:32.732459Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.DataFrame(data).iloc[1:]",
   "id": "ad36861bc523b331",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T10:04:34.304700Z",
     "start_time": "2025-07-18T10:04:34.298707Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "c2328ca687351a1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    sequence_length  calculated_flops  estimated_inference_time  \\\n",
       "1               128     1032973880576                  0.016674   \n",
       "2               256     2074537054464                  0.033487   \n",
       "3               512     4183433206016                  0.067529   \n",
       "4              1024     8504304724224                  0.137277   \n",
       "5              2048    17558364621056                  0.283428   \n",
       "6              4096    37315751856384                  0.602353   \n",
       "7              1366    11467075744944                  0.185102   \n",
       "8              5264    49568325125504                  0.800134   \n",
       "9              4677    43321164415656                  0.699292   \n",
       "10             7151    70874568101624                  1.144061   \n",
       "11             5875    56262790016280                  0.908197   \n",
       "\n",
       "    inference_time  time_delta  delta_percentage  \n",
       "1         0.169113    0.152439          0.901401  \n",
       "2         0.176942    0.143455          0.810744  \n",
       "3         0.158545    0.091016          0.574069  \n",
       "4         0.158224    0.020947          0.132387  \n",
       "5         0.158125   -0.125303          0.792434  \n",
       "6         0.184976   -0.417376          2.256377  \n",
       "7         0.194239    0.009137          0.047040  \n",
       "8         0.181893   -0.618241          3.398926  \n",
       "9         0.188178   -0.511115          2.716125  \n",
       "10        0.167521   -0.976540          5.829358  \n",
       "11        0.173350   -0.734847          4.239097  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>calculated_flops</th>\n",
       "      <th>estimated_inference_time</th>\n",
       "      <th>inference_time</th>\n",
       "      <th>time_delta</th>\n",
       "      <th>delta_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>1032973880576</td>\n",
       "      <td>0.016674</td>\n",
       "      <td>0.169113</td>\n",
       "      <td>0.152439</td>\n",
       "      <td>0.901401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>2074537054464</td>\n",
       "      <td>0.033487</td>\n",
       "      <td>0.176942</td>\n",
       "      <td>0.143455</td>\n",
       "      <td>0.810744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512</td>\n",
       "      <td>4183433206016</td>\n",
       "      <td>0.067529</td>\n",
       "      <td>0.158545</td>\n",
       "      <td>0.091016</td>\n",
       "      <td>0.574069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1024</td>\n",
       "      <td>8504304724224</td>\n",
       "      <td>0.137277</td>\n",
       "      <td>0.158224</td>\n",
       "      <td>0.020947</td>\n",
       "      <td>0.132387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2048</td>\n",
       "      <td>17558364621056</td>\n",
       "      <td>0.283428</td>\n",
       "      <td>0.158125</td>\n",
       "      <td>-0.125303</td>\n",
       "      <td>0.792434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4096</td>\n",
       "      <td>37315751856384</td>\n",
       "      <td>0.602353</td>\n",
       "      <td>0.184976</td>\n",
       "      <td>-0.417376</td>\n",
       "      <td>2.256377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1366</td>\n",
       "      <td>11467075744944</td>\n",
       "      <td>0.185102</td>\n",
       "      <td>0.194239</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>0.047040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5264</td>\n",
       "      <td>49568325125504</td>\n",
       "      <td>0.800134</td>\n",
       "      <td>0.181893</td>\n",
       "      <td>-0.618241</td>\n",
       "      <td>3.398926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4677</td>\n",
       "      <td>43321164415656</td>\n",
       "      <td>0.699292</td>\n",
       "      <td>0.188178</td>\n",
       "      <td>-0.511115</td>\n",
       "      <td>2.716125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7151</td>\n",
       "      <td>70874568101624</td>\n",
       "      <td>1.144061</td>\n",
       "      <td>0.167521</td>\n",
       "      <td>-0.976540</td>\n",
       "      <td>5.829358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5875</td>\n",
       "      <td>56262790016280</td>\n",
       "      <td>0.908197</td>\n",
       "      <td>0.173350</td>\n",
       "      <td>-0.734847</td>\n",
       "      <td>4.239097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:11:41.481Z",
     "start_time": "2025-07-21T08:11:41.478010Z"
    }
   },
   "cell_type": "code",
   "source": "seq_len = 4096",
   "id": "eeabf8b4bce770a7",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:11:41.506394Z",
     "start_time": "2025-07-21T08:11:41.503268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_flops = calculate_theoretical_flop_count((1,seq_len), model_config, guessed_tokenization_constant)\n",
    "n_flops"
   ],
   "id": "1027aa79b7b21b44",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83513358407936"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:11:41.525018Z",
     "start_time": "2025-07-21T08:11:41.521504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_approx = 2000 * 8.03 * 1e9\n",
    "n_approx, n_flops/n_approx"
   ],
   "id": "3fec80e9ac8b674c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16059999999999.998, 5.200084583308594)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:11:41.551113Z",
     "start_time": "2025-07-21T08:11:41.548333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t_pf = (2000 * 8.03 * 1e9 / GPU_FLOPS)\n",
    "t_pf"
   ],
   "id": "bc79b9c4d8ee4d45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09721549636803872"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:11:41.576837Z",
     "start_time": "2025-07-21T08:11:41.573950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "t_tot = t_pf + (2 * 8.03 * 1e9 / GPU_BANDWIDTH)\n",
    "t_tot"
   ],
   "id": "e7ae5c3f9923682c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11205383024534486"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:11:41.594995Z",
     "start_time": "2025-07-21T08:11:41.593366Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f7f5c1e6ad04f7f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T08:11:41.632716Z",
     "start_time": "2025-07-21T08:11:41.629581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "estimated_memory_time = .08 * n_flops / GPU_BANDWIDTH\n",
    "estimated_memory_time"
   ],
   "id": "12333ac7bf2d3225",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.17284729898922"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "estimated_compute_time = n_flops / GPU_FLOPS\n",
    "estimated_compute_time"
   ],
   "id": "7efdf691738ce785"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
